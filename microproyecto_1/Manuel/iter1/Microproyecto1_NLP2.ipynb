{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109b36f2-4229-4c2d-a8d5-734fb316b46d",
   "metadata": {},
   "source": [
    "# **Microproyecto 1:** Generación de Notas Musicales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8257ea3e-cec6-4002-812c-52271abc1be8",
   "metadata": {},
   "source": [
    "El objetivo de este Notebook es entrenar un modelo de secuencia que, a partir de un contexto de notas previas (`pitch`, `step`, `duration`, y opcionalmente `velocity`), prediga la siguiente nota con las mismas características, siguiendo el estilo de un compositor y un instrumento de los presentes en el dataset.\n",
    "\n",
    "Presentado por: \n",
    "- Manuel Estévez Bretón - me.estevez-breton10@uniandes.edu.co\n",
    "- Ling Lung Zuñiga - l.lung@uniandes.edu.co\n",
    "- Victoria Orellana Guerrero - v.orellana@uniandes.edu.co\n",
    "- Jorge Paternina Montiel - j.paterninam@uniandes.edu.co \n",
    "- Benjamin Perdomo Morales - b.perdomom@uniandes.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d31a9d-93bb-4da4-b615-88cd05e7d6ae",
   "metadata": {},
   "source": [
    "## **Notebook Set-up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4f9f9efb-e212-4e0e-8df8-777d85c36442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias\n",
    "# %pip install -q pretty_midi ipywidgets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cf0d1f8d-16f3-4a0f-a340-0552237a5d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librarías importadas correctamente. Torch CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# Importaciones básicas\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # Para visualizar barras de progreso\n",
    "\n",
    "# Audio/MIDI\n",
    "import pretty_midi\n",
    "\n",
    "# Deep Learning \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
    "\n",
    "# Reproducibilidad\n",
    "import random\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print('Librarías importadas correctamente. Torch CUDA:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286b991f-74de-432b-807d-30437c1d7dd3",
   "metadata": {},
   "source": [
    "## **Carga de Datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "90c34399-7485-47da-a839-f0223b47dc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron 21 archivos MIDI para el compositor: 'mozart'\n"
     ]
    }
   ],
   "source": [
    "# Ruta principal donde se encuentran las carpetas de los compositores\n",
    "base_path = '../dataset/music_artist'\n",
    "\n",
    "# Path para extraer pistas de un único compositor \n",
    "composer = 'mozart'\n",
    "composer_path = os.path.join(base_path, composer)\n",
    "midi_files = glob.glob(os.path.join(composer_path, '*.mid'))\n",
    "\n",
    "print(f\"Se encontraron {len(midi_files)} archivos MIDI para el compositor: '{composer}'\")\n",
    "#print(\"Ruta de ejemplo:\", midi_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "039fefef-ba97-4894-9a9b-5427b1254455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando instrumentos en los archivos MIDI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analizando archivos: 100%|██████████| 21/21 [00:01<00:00, 14.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Instrumentos Disponibles ---\n",
      "Acoustic Grand Piano\n",
      "\n",
      "El instrumento seleccionado es: Acoustic Grand Piano\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def list_available_instruments(midi_files):\n",
    "    \"\"\"\n",
    "    Analiza una lista de archivos MIDI y devuelve una lista de todos los\n",
    "    nombres de instrumentos únicos encontrados.\n",
    "    \"\"\"\n",
    "    instrument_names = set()\n",
    "\n",
    "    print(\"Buscando instrumentos en los archivos MIDI...\")\n",
    "    for midi_path in tqdm(midi_files, desc=\"Analizando archivos\"):\n",
    "        try:\n",
    "            pm = pretty_midi.PrettyMIDI(midi_path)\n",
    "            for instrument in pm.instruments:\n",
    "                instrument_name = pretty_midi.program_to_instrument_name(instrument.program)\n",
    "                instrument_names.add(instrument_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Ocurrió un error menor en el archivo {midi_path}: {e}\")\n",
    "            continue\n",
    "    return sorted(list(instrument_names))\n",
    "\n",
    "available_instruments = list_available_instruments(midi_files)\n",
    "print(\"\\n--- Instrumentos Disponibles ---\")\n",
    "for instrument in available_instruments:\n",
    "    print(f\"{instrument}\")\n",
    "\n",
    "# Nuestro approach para seleccionar el instrumento será en función de su frecuencia \n",
    "selected_instrument =max(set(available_instruments), key=available_instruments.count) \n",
    "print(f\"\\nEl instrumento seleccionado es: {selected_instrument}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "922a1e64-9bed-47d3-8e59-fea910575a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pitch      step  duration  velocity\n",
      "0     66  0.000000  0.423042        60\n",
      "1     74  0.000000  0.423042        73\n",
      "2     69  0.000000  0.423042        60\n",
      "3     79  0.846084  0.111615        66\n",
      "4     78  0.101468  0.111615        55\n"
     ]
    }
   ],
   "source": [
    "def extract_notes_by_instrument(midi_path, target_instrument_name):\n",
    "    \"\"\"\n",
    "    Función para abrir un archivo MIDI y extraer las notas de un instrumento.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pm = pretty_midi.PrettyMIDI(midi_path)\n",
    "        \n",
    "        instrument_to_extract = None\n",
    "        for instrument in pm.instruments:\n",
    "            instrument_name = pretty_midi.program_to_instrument_name(instrument.program)\n",
    "            # Ahora la comparación usa la variable correcta que pasaste como parámetro\n",
    "            if instrument_name == target_instrument_name:\n",
    "                instrument_to_extract = instrument\n",
    "                break \n",
    "                \n",
    "        if not instrument_to_extract:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        notes = instrument_to_extract.notes\n",
    "        sorted_notes = sorted(notes, key=lambda note: note.start)\n",
    "\n",
    "        prev_start = 0\n",
    "        extracted_notes = []\n",
    "\n",
    "        for note in sorted_notes:\n",
    "            start = note.start\n",
    "            end = note.end\n",
    "            \n",
    "            pitch = note.pitch\n",
    "            step = start - prev_start\n",
    "            duration = end - start\n",
    "            velocity = note.velocity\n",
    "\n",
    "            extracted_notes.append({\n",
    "                'pitch': pitch, 'step': step, \n",
    "                'duration': duration, 'velocity': velocity\n",
    "            })\n",
    "            \n",
    "            prev_start = start\n",
    "\n",
    "        return pd.DataFrame(extracted_notes)\n",
    "\n",
    "    except Exception as e:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "df_test = extract_notes_by_instrument(midi_files[0], selected_instrument) \n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bd3d3859-7dc8-4b2b-adb3-a383bf36beb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando todos los archivos de 'mozart'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo notas: 100%|██████████| 21/21 [00:01<00:00, 14.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de notas extraídas: 38545\n",
      "Primeras 10 filas del dataset completo:\n",
      "   pitch      step  duration  velocity\n",
      "0     66  0.000000  0.423042        60\n",
      "1     74  0.000000  0.423042        73\n",
      "2     69  0.000000  0.423042        60\n",
      "3     79  0.846084  0.111615        66\n",
      "4     78  0.101468  0.111615        55\n",
      "5     76  0.101468  0.111615        54\n",
      "6     78  0.101468  0.111615        54\n",
      "7     81  0.101468  0.111615        65\n",
      "8     79  0.101468  0.112511        55\n",
      "9     78  0.101468  0.121475        55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_composer_notes = []\n",
    "print(f\"\\nProcesando todos los archivos de '{composer}'\")\n",
    "for file in tqdm(midi_files, desc=f\"Extrayendo notas\"):\n",
    "    notes_df = extract_notes_by_instrument(file, instrument)\n",
    "    if not notes_df.empty:\n",
    "        all_composer_notes.append(notes_df)\n",
    "\n",
    "# Concatenamos todos los DataFrames de la lista en uno solo\n",
    "if all_composer_notes:\n",
    "    full_dataset = pd.concat(all_composer_notes, ignore_index=True)\n",
    "    print(f\"Número total de notas extraídas: {len(full_dataset)}\")\n",
    "    print(\"Primeras 10 filas del dataset completo:\")\n",
    "    print(full_dataset.head(10))\n",
    "else:\n",
    "    print(\"\\nNo se pudieron extraer notas de ningún archivo. Revisar rutas o archivos MIDI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24234a3-a9f0-4fc6-9863-0bb7b6715af5",
   "metadata": {},
   "source": [
    "**¿Por qué no incluimos un ID de la pista en el dataset?**\n",
    "Nuestro objetivo es que el modelo aprenda el lenguaje musical de un compositor, no que memorice canciones específicas. Por tal motivo, tratamos toda su obra como un único texto musical. El objetivo es encontrar patrones de su estilo, no sobreajustarnos a una canción. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e17ea16",
   "metadata": {},
   "source": [
    "## **Dataload**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bd72ab",
   "metadata": {},
   "source": [
    "La música clásica, como la de Mozart, suele estar escrita en compases (8 notas). Por ende, se seleccionarán múltiplos de ese valor para la creación de las sequencias. \n",
    "\n",
    "Inicialmente, se comenzará con 32 (4 compases), pues con estos se compone una subfrase melódica. Esto, entonces, le da a la red neuronal suficiente contexto, tatno a nivel armónico como a nivel melódico, para aprender los patrones de las composiciones del artista. De no funcionar adecuadamente, se ajustará este valor a 64 o, si no se logra entrenar en un tiempo adecuado, a 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c7b5b316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38545, 4)\n"
     ]
    }
   ],
   "source": [
    "print(full_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bd7fd84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, seq_lenght):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(df) - seq_lenght):\n",
    "        x = df.iloc[i:(i+seq_lenght), :]\n",
    "        y = df.iloc[i+seq_lenght, :]\n",
    "\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "27c0e233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30836, 4)\n",
      "(7709, 4)\n",
      "38545\n"
     ]
    }
   ],
   "source": [
    "train_data = full_dataset.copy()\n",
    "test_data = full_dataset.copy()\n",
    "\n",
    "train_size_threshold = int(full_dataset.shape[0] * 0.8)\n",
    "\n",
    "train_data = train_data.iloc[:train_size_threshold , :]\n",
    "test_data = test_data.iloc[train_size_threshold: , :]\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_data.shape[0] + test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "595b591d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30708, 128, 4) (30708, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = create_sequences(train_data, 128)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "062f1228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7581, 128, 4) (7581, 4)\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = create_sequences(test_data, 128)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a89401ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TensorDataset(\n",
    "    torch.from_numpy(X_train).float(),\n",
    "    torch.from_numpy(y_train).float()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0566c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = TensorDataset(\n",
    "    torch.from_numpy(X_test).float(),\n",
    "    torch.from_numpy(y_test).float()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411f605-6add-4581-b4dc-8ed5f7cc6988",
   "metadata": {},
   "source": [
    "## **Arquitectura GRU**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd0339a",
   "metadata": {},
   "source": [
    "### Instanciamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b68f13b-56f4-479c-9f67-e731ecf507b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, hn = self.gru(x, h0)\n",
    "\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d899188c",
   "metadata": {},
   "source": [
    "### Traning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5725d44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start on cuda\n",
      "epoch [1/15], Loss: 220.0514\n",
      "epoch [2/15], Loss: 49.3249\n",
      "epoch [3/15], Loss: 49.1831\n",
      "epoch [4/15], Loss: 49.5430\n",
      "epoch [5/15], Loss: 49.4478\n",
      "epoch [6/15], Loss: 42.3398\n",
      "epoch [7/15], Loss: 26.3052\n",
      "epoch [8/15], Loss: 23.8187\n",
      "epoch [9/15], Loss: 23.2539\n",
      "epoch [10/15], Loss: 22.6975\n",
      "epoch [11/15], Loss: 20.2508\n",
      "epoch [12/15], Loss: 19.0903\n",
      "epoch [13/15], Loss: 18.9162\n",
      "epoch [14/15], Loss: 18.5693\n",
      "epoch [15/15], Loss: 18.1803\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "INPUT_FEATURES = 4\n",
    "OUTPUT_FEATURES = 4\n",
    "HIDDEN_UNITS = 512\n",
    "\n",
    "model_gru = GRUNet(\n",
    "    input_size=INPUT_FEATURES,\n",
    "    hidden_size=HIDDEN_UNITS,\n",
    "    output_size=OUTPUT_FEATURES\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_gru.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_gru.to(device)\n",
    "model_gru.train()\n",
    "\n",
    "print(f'Training start on {device}')\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for seqs, labels in dataloader_train:\n",
    "        seqs, labels = seqs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_gru(seqs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * seqs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataset_train)\n",
    "    print(f'epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76da8c7",
   "metadata": {},
   "source": [
    "### Evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "efefdbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Finished.\n",
      "Average MSE Loss: 11.1036\n",
      "Root Mean Squared Error (RMSE): 3.3322\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    \n",
    "    model.eval() \n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seqs, labels in data_loader:\n",
    "            seqs, labels = seqs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(seqs)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * seqs.size(0)\n",
    "            \n",
    "            # Store results\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_targets.append(labels.cpu().numpy())\n",
    "\n",
    "    # Aggregate results\n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    predictions = np.concatenate(all_preds, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # Calculate Root Mean Squared Error (RMSE) for better interpretability\n",
    "    # RMSE is the square root of MSE\n",
    "    rmse = np.sqrt(mean_squared_error(targets, predictions))\n",
    "    \n",
    "    return avg_loss, rmse\n",
    "\n",
    "# --- Run Evaluation ---\n",
    "val_mse_loss, val_rmse = evaluate_model(model_gru, dataloader_test, criterion, device)\n",
    "\n",
    "print(f\"Validation Finished.\")\n",
    "print(f\"Average MSE Loss: {val_mse_loss:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {val_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81922c4",
   "metadata": {},
   "source": [
    "### **Generación de música**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2663c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notes(model, initial_seed, num_notes_to_generate, device, \n",
    "                   max_pitch=127, min_pitch=0, \n",
    "                   max_velocity=127, min_velocity=1):\n",
    "    \"\"\"\n",
    "    Generates a sequence of notes using the trained GRU model autoregressively.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained GRU model.\n",
    "        initial_seed (torch.Tensor): A starting sequence of notes (32, 4).\n",
    "        num_notes_to_generate (int): The total number of notes to generate.\n",
    "        device (torch.device): 'cuda' or 'cpu'.\n",
    "        max_pitch, min_pitch, etc.: Limits for note feature clipping/scaling.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of the 4-feature numpy arrays for each generated note.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    \n",
    "    # current_sequence shape: (1, 32, 4) - Batch size of 1\n",
    "    current_sequence = initial_seed.unsqueeze(0).to(device)\n",
    "    \n",
    "    generated_notes_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_notes_to_generate):\n",
    "            # 1. Prediction: Get the 4-feature output (pitch, step, duration, velocity)\n",
    "            # output shape: (1, 4)\n",
    "            predicted_note_tensor = model(current_sequence)\n",
    "            \n",
    "            # Convert prediction to numpy and squeeze out the batch dimension\n",
    "            pred_note_np = predicted_note_tensor.squeeze(0).cpu().numpy()\n",
    "            \n",
    "            # 2. Post-processing/Discretization:\n",
    "            # We must convert continuous predictions into valid note parameters.\n",
    "            \n",
    "            # a. Pitch and Velocity are integers [0-127]. We'll round and clip.\n",
    "            pred_note_np[0] = np.clip(np.round(pred_note_np[0]), min_pitch, max_pitch)    # Pitch\n",
    "            pred_note_np[3] = np.clip(np.round(pred_note_np[3]), min_velocity, max_velocity) # Velocity\n",
    "\n",
    "            # b. Step and Duration are time-based (float), we can clip to positive values.\n",
    "            pred_note_np[1] = np.clip(pred_note_np[1], 0.01, None) # Step (must be > 0)\n",
    "            pred_note_np[2] = np.clip(pred_note_np[2], 0.01, None) # Duration (must be > 0)\n",
    "            \n",
    "            # Store the generated note (4 features)\n",
    "            generated_notes_list.append(pred_note_np)\n",
    "            \n",
    "            # 3. Feedback: Prepare the input for the next prediction\n",
    "            \n",
    "            # Convert the new note back to a tensor (1, 1, 4)\n",
    "            new_note_tensor = torch.from_numpy(pred_note_np).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Append the new note and drop the oldest note\n",
    "            # new_sequence shape: (1, 32, 4)\n",
    "            current_sequence = torch.cat((current_sequence[:, 1:, :], new_note_tensor), dim=1)\n",
    "            \n",
    "    return generated_notes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "77f219ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train is your original (30804, 32, 4) data tensor\n",
    "# Ensure these indices are far apart to get distinct musical starting points.\n",
    "seed_index_1 = 0    # First 32 notes\n",
    "seed_index_2 = 8000 # A sequence later in the dataset\n",
    "seed_index_3 = 15000 # Another sequence much later\n",
    "\n",
    "seed_1 = X_train[seed_index_1] # shape (32, 4)\n",
    "seed_2 = X_train[seed_index_2] # shape (32, 4)\n",
    "seed_3 = X_train[seed_index_3] # shape (32, 4)\n",
    "\n",
    "# Create a list of seeds\n",
    "seeds = [seed_1, seed_2, seed_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "53d0d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def notes_to_audio_file(generated_notes, output_path, instrument_name='Acoustic Grand Piano'):\n",
    "    \"\"\"\n",
    "    Converts a list of generated notes (4 features each) into a WAV file.\n",
    "    \n",
    "    Args:\n",
    "        generated_notes (list): List of 4-feature numpy arrays (pitch, step, duration, velocity).\n",
    "        output_path (str): Path to save the output file (e.g., 'song_1.wav').\n",
    "        instrument_name (str): MIDI instrument name.\n",
    "    \"\"\"\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    \n",
    "    try:\n",
    "        program = pretty_midi.instrument_name_to_program(instrument_name)\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Instrument '{instrument_name}' not found. Using default.\")\n",
    "        program = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')\n",
    "        \n",
    "    instrument = pretty_midi.Instrument(program=program)\n",
    "    \n",
    "    current_time = 0.0 # Start time of the first note\n",
    "    \n",
    "    for note_features in generated_notes:\n",
    "        pitch, step, duration, velocity = [int(note_features[0]), \n",
    "                                          note_features[1], \n",
    "                                          note_features[2], \n",
    "                                          int(note_features[3])]\n",
    "        \n",
    "        # 1. Calculate Start Time: Advance the time by the 'step' (time since last note ended)\n",
    "        current_time += step \n",
    "        start = current_time\n",
    "        \n",
    "        # 2. Calculate End Time: Add the 'duration' to the start time\n",
    "        end = start + duration\n",
    "        \n",
    "        # 3. Create the PrettyMIDI Note object\n",
    "        note = pretty_midi.Note(velocity=velocity, pitch=pitch, start=start, end=end)\n",
    "        \n",
    "        instrument.notes.append(note)\n",
    "        \n",
    "        # 4. Update the time pointer: The next note's 'step' is added from *this* note's end\n",
    "        current_time = end \n",
    "\n",
    "    pm.instruments.append(instrument)\n",
    "    \n",
    "    # Save as a MIDI file first (PrettyMIDI's primary output)\n",
    "    midi_path = output_path.replace('.wav', '.mid')\n",
    "    pm.write(midi_path)\n",
    "    print(f\"MIDI file saved to: {midi_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6fc3a49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Song 1 with Seed 1 ---\n",
      "MIDI file saved to: generated_song_1.mid\n",
      "\n",
      "--- Generating Song 2 with Seed 2 ---\n",
      "MIDI file saved to: generated_song_2.mid\n",
      "\n",
      "--- Generating Song 3 with Seed 3 ---\n",
      "MIDI file saved to: generated_song_3.mid\n",
      "\n",
      "All 3 songs generated successfully (MIDI). Check the console for WAV rendering status.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup (Re-run this part with the fix) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_NOTES = 200\n",
    "output_paths = ['generated_song_1.wav', 'generated_song_2.wav', 'generated_song_3.wav']\n",
    "\n",
    "# Assuming seeds are defined as a list of NumPy arrays:\n",
    "# seeds = [seed_1, seed_2, seed_3] \n",
    "\n",
    "for i, seed_np in enumerate(seeds):\n",
    "    print(f\"\\n--- Generating Song {i+1} with Seed {i+1} ---\")\n",
    "    \n",
    "    # FIX: Convert the NumPy array seed to a PyTorch Tensor\n",
    "    seed_tensor = torch.from_numpy(seed_np).float() \n",
    "    \n",
    "    # 1. Generate the notes\n",
    "    # Pass the PyTorch tensor to the generation function\n",
    "    generated_notes = generate_notes(model_gru, seed_tensor, NUM_NOTES, device)\n",
    "    \n",
    "    # 2. Convert to WAV file\n",
    "    notes_to_audio_file(generated_notes, output_paths[i])\n",
    "\n",
    "print(\"\\nAll 3 songs generated successfully (MIDI). Check the console for WAV rendering status.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce055548-8e22-4ebc-9b56-5689f2c71d96",
   "metadata": {},
   "source": [
    "## **Arquitectura LSTM (Long Short-Term Memory)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c4f370-77e0-4e78-bcb4-df3eb3a7a643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
