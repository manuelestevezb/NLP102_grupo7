{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac25240",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEQ_LEN       = 50\n",
    "SPLIT         = 0.8\n",
    "LOSS_W_CONT   = 0.5\n",
    "TEMP          = 1.0\n",
    "NOISE_STD     = 0.05\n",
    "CAP_STEP      = 2.0\n",
    "CAP_DURATION  = 2.0\n",
    "SHOW_PPL      = True\n",
    "USE_SCHEDULER = True\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pretty_midi\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def extract_features(midi_file):\n",
    "    pm = pretty_midi.PrettyMIDI(midi_file)\n",
    "    inst = pm.instruments[0]\n",
    "    notes = inst.notes\n",
    "\n",
    "    pitches, steps, durations, velocities = [], [], [], []\n",
    "    prev_time = 0.0\n",
    "\n",
    "    for i, note in enumerate(notes):\n",
    "        pitches.append(note.pitch)\n",
    "        start = note.start\n",
    "        dur = note.end - start\n",
    "        vel = note.velocity\n",
    "        step = start - prev_time if i > 0 else 0.0\n",
    "        steps.append(step); durations.append(dur); velocities.append(vel)\n",
    "        prev_time = start\n",
    "\n",
    "    steps = torch.tensor(steps, dtype=torch.float32)\n",
    "    durations = torch.tensor(durations, dtype=torch.float32)\n",
    "    velocities = torch.tensor(velocities, dtype=torch.float32)\n",
    "    pitches = torch.tensor(pitches, dtype=torch.long)\n",
    "\n",
    "    step_max = steps.max().item() + 0.00001\n",
    "    duration_max = durations.max().item() + 0.00001\n",
    "\n",
    "    steps = steps / step_max\n",
    "    durations = durations / duration_max\n",
    "    velocities = velocities / 127.0\n",
    "\n",
    "\n",
    "    cont_features = torch.stack([steps, durations, velocities], dim=1)\n",
    "    return pitches, cont_features, step_max, duration_max\n",
    "\n",
    "\n",
    "def load_all_data(dataset_dir):\n",
    "    pitches_all, cont_all = [], []\n",
    "    step_max_global, duration_max_global = 0.0, 0.0\n",
    "\n",
    "    for filename in sorted(os.listdir(dataset_dir)):\n",
    "        if filename.lower().endswith(('.mid', '.midi')):\n",
    "            p, c, step_max, duration_max = extract_features(os.path.join(dataset_dir, filename))\n",
    "            pitches_all.append(p); cont_all.append(c)\n",
    "            step_max_global = max(step_max_global, step_max)\n",
    "            duration_max_global = max(duration_max_global, duration_max)\n",
    "\n",
    "    pitches_all = torch.cat(pitches_all)\n",
    "    cont_all = torch.cat(cont_all)\n",
    "    return pitches_all, cont_all, step_max_global, duration_max_global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83246db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(pitches, cont_features, seq_length):\n",
    "    pitch_seqs, cont_seqs, pitch_targets, cont_targets = [], [], [], []\n",
    "    for i in range(len(pitches) - seq_length):\n",
    "        pitch_seqs.append(pitches[i:i + seq_length])\n",
    "        cont_seqs.append(cont_features[i:i + seq_length])\n",
    "        pitch_targets.append(pitches[i + seq_length])\n",
    "        cont_targets.append(cont_features[i + seq_length])\n",
    "\n",
    "    return (torch.stack(pitch_seqs),\n",
    "            torch.stack(cont_seqs),\n",
    "            torch.tensor(pitch_targets),\n",
    "            torch.stack(cont_targets))\n",
    "\n",
    "\n",
    "def temporal_split_before_window(pitches, cont, split_ratio=0.8):\n",
    "    n_total = len(pitches)\n",
    "    cut = int(n_total * split_ratio)\n",
    "    return (pitches[:cut], cont[:cut]), (pitches[cut:], cont[cut:])\n",
    "\n",
    "\n",
    "class MusicLSTMMultiOutput(nn.Module):\n",
    "    def __init__(self, n_pitches=128, embed_size=32, hidden_size=128, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(n_pitches, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size + 3, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.hidden_fc = nn.Linear(hidden_size, hidden_size)\n",
    "        self.pitch_fc = nn.Linear(hidden_size, n_pitches)\n",
    "        self.cont_fc = nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def forward(self, pitch_seq, cont_seq):\n",
    "        pitch_emb = self.embed(pitch_seq)\n",
    "        x = torch.cat([pitch_emb, cont_seq], dim=2)\n",
    "        out, _ = self.lstm(x)\n",
    "        h = F.relu(self.hidden_fc(out[:, -1, :]))\n",
    "        return self.pitch_fc(h), self.cont_fc(h)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
